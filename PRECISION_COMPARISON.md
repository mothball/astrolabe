# Precision Comparison Results

## Benchmark Hardware
- **CPU**: AMD Ryzen 9 9950X3D
- **SIMD**: AVX-512 (8-wide Float64, theoretically 16-wide Float32)
- **Test**: 10,000 satellites √ó 10 timesteps = 100,000 propagations

## Results Summary

| Precision Mode | Performance | vs FP64 | Accuracy | Status |
|:---|---:|---:|:---|:---|
| **FP64 (baseline)** | **212,800,812 props/sec** | 1.00x | Machine precision (~1e-15) | ‚úÖ Deployed |
| **FP32** | 85,685,475 props/sec | **0.40x** ‚ö†Ô∏è | ~1e-7 | ‚úÖ Tested |
| **Mixed Precision** | (Not implemented) | ~0.5-0.7x (est.) | ~1e-13 (est.) | ‚ùå Not started |

## Key Findings

### 1. FP32 is **2.5x SLOWER** than FP64 ‚ùå

**Theoretical expectation**: FP32 should be 1.5-2x faster (double SIMD width: 16-wide vs 8-wide)

**Actual result**: FP32 is 2.5x **slower**

**Reasons**:
1. **Memory bandwidth bottleneck**: Still loading same amount of data (Float64 from arrays)
2. **Constant casting overhead**: Every `KE`, `KMPER`, `CK2`, etc. needs `Float32()` cast
3. **Type conversion**: Cast FP64 ‚Üí FP32 on load, FP32 ‚Üí FP64 on store
4. **Modern CPU optimization**: Ryzen 9 has heavily optimized FP64 units
5. **No fast_math**: Had to use standard `sin/cos` (slower than optimized FP64 `fast_sin_cos`)

### 2. Why FP64 is faster on modern hardware

Modern server CPUs (Zen 4, Ice Lake, etc.) have:
- **Same-width FP32 and FP64 SIMD units** (both 512-bit)
- **Equal throughput** for FP32 and FP64 operations
- **Heavily optimized FP64** paths (scientific computing focus)
- **Memory bandwidth** as the bottleneck (not computation)

### 3. Mixed Precision (not implemented)

**Theory**: Use FP32 for compute, FP64 for final results
- **Pros**: Reduce casting, maintain accuracy where needed
- **Cons**: Complex implementation, likely still slower due to overhead
- **Est. speedup**: 0.5-0.7x (still slower than pure FP64)

## Comparison to Other Implementations

| Implementation | Performance | Notes |
|:---|---:|:---|
| **Mojo FP64 (our baseline)** | **213M props/sec** | Current champion |
| Mojo FP32 | 86M props/sec | Unexpectedly slow |
| Heyoka (FP64) | 51M props/sec | 4.2x slower than our FP64 |
| SGP4 Python | 322 props/sec | 660,000x slower |

## Recommendations

### ‚ùå Do NOT use FP32
- No performance benefit
- Loss of accuracy
- Implementation complexity

### ‚úÖ Stick with FP64
- Already world-class performance
- Machine precision
- Clean implementation

### üöÄ Focus on GPU instead
- GPU has **separate FP32/FP64 units**
- FP32 on GPU is often 2-32x faster than FP64
- Already have working GPU implementation
- Expected: 5-20 **billion** props/sec

## Architectural Insights

### Why theoretical doesn't match reality:

**Theory (old CPUs)**:
```
FP32: 16 operations √ó 1 cycle = 16 ops/cycle
FP64:  8 operations √ó 1 cycle =  8 ops/cycle
Speedup: 2x
```

**Reality (modern CPUs)**:
```
FP32: 8 operations √ó 1 cycle + overhead = 0.4x
FP64: 8 operations √ó 1 cycle = 1.0x  
Actual: FP32 is slower!
```

### Memory bandwidth analysis:
```
Input:  10K satellites √ó 7 params √ó 8 bytes = 560 KB
Output: 100K results √ó 6 values √ó 8 bytes = 4,800 KB
Total:  ~5.3 MB per benchmark run

FP32 loads same 5.3 MB (Float64 inputs)
‚Üí No bandwidth savings
‚Üí Only downside: extra casting
```

## Conclusion

**FP32 precision reduction is NOT beneficial** for CPU-based SGP4 on modern hardware.

The **current FP64 implementation at 213M props/sec** is:
- ‚úÖ 4.2x faster than Heyoka
- ‚úÖ Machine precision accurate  
- ‚úÖ Clean, maintainable code
- ‚úÖ **Already world-class**

**Next step**: GPU implementation for 10-50x gains, where FP32 actually helps.
